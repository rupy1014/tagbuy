"""
TagBy.io ì¸í”Œë£¨ì–¸ì„œ í¬ë¡¤ëŸ¬

API ê¸°ë°˜ìœ¼ë¡œ ì¸í”Œë£¨ì–¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
"""

import requests
import json
import time
from typing import Optional, Dict, List, Any
from dataclasses import dataclass, asdict
from datetime import datetime


@dataclass
class Influencer:
    """ì¸í”Œë£¨ì–¸ì„œ ë°ì´í„° ëª¨ë¸"""
    username: str
    platform: str
    account_type: str  # ê°œì¸/í”„ë¡œí˜ì…”ë„
    categories: List[str]
    gender: Optional[str]
    age_group: Optional[str]
    follower_count: int
    avg_reach: int
    influence_score: int
    ad_rate: float  # ê´‘ê³ ì§€ìˆ˜ (%)
    instagram_url: str
    raw_data: Dict[str, Any]  # ì›ë³¸ API ì‘ë‹µ


class TagbyCrawler:
    """TagBy.io API í¬ë¡¤ëŸ¬"""

    BASE_URL = "https://api.tagby.io"
    ASYNC_URL = "https://async.tagby.io"

    def __init__(self, email: str, password: str):
        self.email = email
        self.password = password
        self.session = requests.Session()
        self.access_token: Optional[str] = None
        self.refresh_token: Optional[str] = None

        # ê³µí†µ í—¤ë” ì„¤ì •
        self.session.headers.update({
            "Accept": "application/json",
            "Content-Type": "application/json",
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Origin": "https://www.tagby.io",
            "Referer": "https://www.tagby.io/",
        })

    def login(self) -> bool:
        """ë¡œê·¸ì¸ ë° í† í° íšë“"""
        url = f"{self.BASE_URL}/auth/login/biz/"

        payload = {
            "email": self.email,
            "password": self.password
        }

        try:
            response = self.session.post(url, json=payload)
            response.raise_for_status()

            data = response.json()
            self.access_token = data["access_token"]["token"]
            self.refresh_token = data["refresh_token"]["token"]

            # ì¸ì¦ í—¤ë” ì„¤ì •
            self.session.headers["Authorization"] = f"Token {self.access_token}"

            # ì¿ í‚¤ í™•ì¸
            print(f"âœ… ë¡œê·¸ì¸ ì„±ê³µ: {self.email}")
            print(f"   í† í° ë§Œë£Œ: {data['access_token']['expires_at']}")
            print(f"   ì¿ í‚¤: {dict(self.session.cookies)}")
            return True

        except requests.RequestException as e:
            print(f"âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
            return False

    def get_me(self) -> Optional[Dict]:
        """í˜„ì¬ ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ"""
        url = f"{self.BASE_URL}/biz/v2/me/"

        try:
            response = self.session.get(url)
            response.raise_for_status()
            return response.json()["data"]
        except requests.RequestException as e:
            print(f"âŒ ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def search_influencers_v1(
        self,
        page: int = 1,
        service: Optional[str] = None,  # INSTA_BIZ, INSTA_BASIC, YOUTUBE, BLOG
        ordering: str = "-follower_count",  # -follower_count, -isi, -avg_reach
        follower_min: Optional[int] = None,
        follower_max: Optional[int] = None,
    ) -> Dict[str, Any]:
        """
        ì¸í”Œë£¨ì–¸ì„œ ê²€ìƒ‰ (api.tagby.io/sns/ API)

        Args:
            page: í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
            service: í”Œë«í¼ í•„í„° (INSTA_BIZ, INSTA_BASIC, YOUTUBE, BLOG)
            ordering: ì •ë ¬ ê¸°ì¤€ (-follower_count, -isi, follower_count ë“±)
            follower_min: ìµœì†Œ íŒ”ë¡œì›Œ
            follower_max: ìµœëŒ€ íŒ”ë¡œì›Œ
        """
        url = f"{self.BASE_URL}/sns/"

        params = {
            "page": page,
            "ordering": ordering,
        }

        # ì„ íƒì  í•„í„° ì¶”ê°€
        if service:
            params["service"] = service
        if follower_min is not None:
            params["follower_count__gte"] = follower_min
        if follower_max is not None:
            params["follower_count__lte"] = follower_max

        try:
            response = self.session.get(url, params=params)
            response.raise_for_status()
            data = response.json()

            # API ì‘ë‹µ í˜•ì‹ ë³€í™˜
            return {
                "items": data.get("results", []),
                "total": data.get("count", 0),
                "next": data.get("next"),
                "previous": data.get("previous"),
            }

        except requests.RequestException as e:
            print(f"âŒ ì¸í”Œë£¨ì–¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"   Status: {e.response.status_code}")
                print(f"   Response: {e.response.text[:500]}")
            return {"items": [], "total": 0}

    def search_influencers(
        self,
        page: int = 1,
        size: int = 20,
        services: List[str] = None,
        sort_by: str = "isi",  # isi: ì˜í–¥ë ¥ì§€ìˆ˜
        sort_order: str = "desc",
        follower_min: Optional[int] = None,
        follower_max: Optional[int] = None,
        birth_min: Optional[str] = None,  # ìƒë…„ì›”ì¼ ìµœì†Œ (YYYY-MM-DD)
        birth_max: Optional[str] = None,  # ìƒë…„ì›”ì¼ ìµœëŒ€ (YYYY-MM-DD)
        categories: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        ì¸í”Œë£¨ì–¸ì„œ ê²€ìƒ‰ (async.tagby.io API v2)

        Args:
            page: í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
            size: í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜
            services: í”Œë«í¼ ["INSTA_BIZ", "INSTA_BASIC"]
            sort_by: ì •ë ¬ ê¸°ì¤€ (isi: ì˜í–¥ë ¥ì§€ìˆ˜)
            sort_order: ì •ë ¬ ìˆœì„œ (desc, asc)
            follower_min: ìµœì†Œ íŒ”ë¡œì›Œ
            follower_max: ìµœëŒ€ íŒ”ë¡œì›Œ
            birth_min: ìƒë…„ì›”ì¼ ìµœì†Œ (ì˜ˆ: 2005-12-17)
            birth_max: ìƒë…„ì›”ì¼ ìµœëŒ€ (ì˜ˆ: 2010-12-17)
            categories: ì¹´í…Œê³ ë¦¬ í•„í„°
        """
        url = f"{self.ASYNC_URL}/sns/search/sns-info"

        # ê¸°ë³¸ ì„œë¹„ìŠ¤ ì„¤ì •
        if services is None:
            services = ["INSTA_BIZ", "INSTA_BASIC"]

        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° êµ¬ì„± (ë¦¬ìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°ëŠ” ë³„ë„ ì²˜ë¦¬)
        params = []
        for svc in services:
            params.append(("services", svc))
        params.append(("states", "ACTIVE"))
        params.append(("states", "RELOGIN"))
        params.append(("only_naver_influencer", "false"))
        params.append(("sort_by", sort_by))
        params.append(("sort_order", sort_order))
        params.append(("page", str(page)))
        params.append(("size", str(size)))

        # íŒ”ë¡œì›Œ ë²”ìœ„
        if follower_min is not None or follower_max is not None:
            f_min = follower_min if follower_min is not None else 0
            f_max = follower_max if follower_max is not None else 999999999
            params.append(("follower_count_ranges", f"{f_min},{f_max}"))

        # ìƒë…„ì›”ì¼ ë²”ìœ„
        if birth_min or birth_max:
            b_min = birth_min or "1900-01-01"
            b_max = birth_max or "2020-12-31"
            params.append(("birth_ranges", f"{b_min},{b_max}"))

        # ì¹´í…Œê³ ë¦¬
        if categories:
            for cat in categories:
                params.append(("categories", cat))

        # async.tagby.io ì „ìš© í—¤ë” ì„¤ì •
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "x-api-version": "v2",
            "Accept": "application/json, text/plain, */*",
            "Referer": "https://tagby.io/",
        }

        try:
            response = requests.get(url, params=params, headers=headers)
            response.raise_for_status()
            data = response.json()

            # ì‘ë‹µ êµ¬ì¡° ë³€í™˜: {code, msg, data: {count, size, page, snsList}}
            if data.get("code") == 0 and "data" in data:
                return {
                    "items": data["data"].get("snsList", []),
                    "total": data["data"].get("count", 0),
                    "page": data["data"].get("page", 1),
                    "size": data["data"].get("size", 20),
                }
            return data

        except requests.RequestException as e:
            print(f"âŒ ì¸í”Œë£¨ì–¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"   Status: {e.response.status_code}")
                print(f"   Response: {e.response.text[:500]}")
            return {"items": [], "total": 0}

    def crawl_all_influencers_v1(
        self,
        max_pages: int = 10,
        delay: float = 0.5,
        **filters
    ) -> List[Dict]:
        """
        ì „ì²´ ì¸í”Œë£¨ì–¸ì„œ í¬ë¡¤ë§ (api.tagby.io/sns/ API)

        Args:
            max_pages: ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜
            delay: ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)
            **filters: ê²€ìƒ‰ í•„í„° (search_influencers_v1 íŒŒë¼ë¯¸í„°)

        Returns:
            ì¸í”Œë£¨ì–¸ì„œ ëª©ë¡
        """
        all_influencers = []

        for page in range(1, max_pages + 1):
            print(f"\nğŸ“„ í˜ì´ì§€ {page}/{max_pages} ìƒˆë¡œê³ ì¹¨ ì¤‘...")

            result = self.search_influencers_v1(page=page, **filters)

            items = result.get("items", [])
            total = result.get("total", 0)

            if not items:
                print(f"   ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break

            all_influencers.extend(items)
            print(f"   âœ… {len(items)}ëª… ìˆ˜ì§‘ (ì´ {len(all_influencers)}/{total}ëª…)")

            # ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸
            if result.get("next") is None:
                print(f"\nğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
                break

            # Rate limiting
            if page < max_pages:
                time.sleep(delay)

        return all_influencers

    def crawl_all_influencers(
        self,
        max_pages: int = 10,
        size: int = 20,
        delay: float = 1.0,
        **filters
    ) -> List[Dict]:
        """
        ì „ì²´ ì¸í”Œë£¨ì–¸ì„œ í¬ë¡¤ë§ (async.tagby.io API)

        Args:
            max_pages: ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜
            size: í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜
            delay: ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)
            **filters: ê²€ìƒ‰ í•„í„° (search_influencers íŒŒë¼ë¯¸í„°)

        Returns:
            ì¸í”Œë£¨ì–¸ì„œ ëª©ë¡
        """
        all_influencers = []

        for page in range(1, max_pages + 1):
            print(f"\nğŸ“„ í˜ì´ì§€ {page}/{max_pages} ìƒˆë¡œê³ ì¹¨ ì¤‘...")

            result = self.search_influencers(page=page, size=size, **filters)

            items = result.get("items", [])
            total = result.get("total", 0)

            if not items:
                print(f"   ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break

            all_influencers.extend(items)
            print(f"   âœ… {len(items)}ëª… ìˆ˜ì§‘ (ì´ {len(all_influencers)}/{total}ëª…)")

            # ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸
            if len(all_influencers) >= total:
                print(f"\nğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
                break

            # Rate limiting
            if page < max_pages:
                time.sleep(delay)

        return all_influencers

    def parse_influencer(self, raw: Dict) -> Influencer:
        """API ì‘ë‹µì„ Influencer ê°ì²´ë¡œ íŒŒì‹±"""

        # íŒ”ë¡œì›Œ ìˆ˜ íŒŒì‹± (ì˜ˆ: "105.4ë§Œ" -> 1054000)
        def parse_count(value) -> int:
            if isinstance(value, int):
                return value
            if isinstance(value, str):
                value = value.replace(",", "")
                if "ë§Œ" in value:
                    return int(float(value.replace("ë§Œ", "")) * 10000)
                return int(value) if value.isdigit() else 0
            return 0

        username = raw.get("username", raw.get("sns_id", ""))

        return Influencer(
            username=username,
            platform="instagram",
            account_type=raw.get("account_type", "unknown"),
            categories=raw.get("categories", []),
            gender=raw.get("gender"),
            age_group=raw.get("age_group"),
            follower_count=parse_count(raw.get("follower_count", 0)),
            avg_reach=parse_count(raw.get("avg_reach", 0)),
            influence_score=parse_count(raw.get("isi", raw.get("influence_score", 0))),
            ad_rate=float(raw.get("ad_rate", 0)),
            instagram_url=f"https://www.instagram.com/{username}/",
            raw_data=raw
        )

    def save_to_json(self, influencers: List[Dict], filename: str = None):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"tagby_influencers_{timestamp}.json"

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(influencers, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename} ({len(influencers)}ëª…)")
        return filename


def main_deep_pagination():
    """ê¹Šì€ í˜ì´ì§•ìœ¼ë¡œ ìµœëŒ€í•œ ë§ì€ ì¸í”Œë£¨ì–¸ì„œ ìˆ˜ì§‘"""

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = TagbyCrawler(
        email="rupy1008@gmail.com",
        password="KM1178km!@"
    )

    if not crawler.login():
        return

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    try:
        with open("tagby_instagram_influencers.json", "r", encoding="utf-8") as f:
            existing = json.load(f)
        all_influencers = {inf.get("username"): inf for inf in existing}
        print(f"ğŸ“‚ ê¸°ì¡´ ë°ì´í„°: {len(all_influencers)}ëª…")
    except:
        all_influencers = {}
        print("ğŸ“‚ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ")

    total_requests = 0
    initial_count = len(all_influencers)

    print("\n" + "="*60)
    print("ğŸš€ ê¹Šì€ í˜ì´ì§•ìœ¼ë¡œ ì¸í”Œë£¨ì–¸ì„œ ìˆ˜ì§‘")
    print("="*60)

    # 1. í•„í„° ì—†ì´ ì „ì²´ í˜ì´ì§• (ê° ì •ë ¬ë³„ ìµœëŒ€ 100í˜ì´ì§€)
    print("\nğŸ”¹ 1ë‹¨ê³„: ì „ì²´ ë°ì´í„° ê¹Šì€ í˜ì´ì§•")
    print("-" * 50)

    sort_options = [
        ("isi", "desc"),
        ("isi", "asc"),
        ("follower_count", "desc"),
        ("follower_count", "asc"),
        ("avg_reach", "desc"),
        ("avg_reach", "asc"),
        ("ad_rate", "desc"),
        ("ad_rate", "asc"),
    ]

    for sort_by, sort_order in sort_options:
        print(f"\nğŸ“Š ì •ë ¬: {sort_by} {sort_order}")
        consecutive_no_new = 0

        for page in range(1, 101):  # ìµœëŒ€ 100í˜ì´ì§€
            result = crawler.search_influencers(
                page=page,
                size=50,
                sort_by=sort_by,
                sort_order=sort_order,
            )

            items = result.get("items", [])
            total = result.get("total", 0)
            total_requests += 1

            if not items:
                print(f"   í˜ì´ì§€ {page}: ë°ì´í„° ì—†ìŒ (ì¢…ë£Œ)")
                break

            new_count = 0
            for inf in items:
                username = inf.get("username")
                if username and username not in all_influencers:
                    all_influencers[username] = inf
                    new_count += 1

            if new_count > 0:
                print(f"   í˜ì´ì§€ {page}: +{new_count}ëª… (ì´ {len(all_influencers)}ëª…)")
                consecutive_no_new = 0
            else:
                consecutive_no_new += 1

            # 3í˜ì´ì§€ ì—°ì† ì‹ ê·œ ì—†ìœ¼ë©´ ë‹¤ìŒ ì •ë ¬ë¡œ
            if consecutive_no_new >= 3:
                print(f"   3í˜ì´ì§€ ì—°ì† ì‹ ê·œ ì—†ìŒ, ë‹¤ìŒ ì •ë ¬ë¡œ")
                break

            if len(items) < 50:
                print(f"   ë§ˆì§€ë§‰ í˜ì´ì§€ (í˜ì´ì§€ {page})")
                break

            time.sleep(0.1)

    # 2. íŒ”ë¡œì›Œ ë²”ìœ„ë³„ ì„¸ë¶„í™” í˜ì´ì§•
    print("\n\nğŸ”¹ 2ë‹¨ê³„: íŒ”ë¡œì›Œ ë²”ìœ„ë³„ ì„¸ë¶„í™”")
    print("-" * 50)

    follower_ranges = [
        (0, 500), (500, 1000), (1000, 2000), (2000, 3000), (3000, 5000),
        (5000, 7000), (7000, 10000), (10000, 15000), (15000, 20000),
        (20000, 30000), (30000, 40000), (40000, 50000), (50000, 70000),
        (70000, 100000), (100000, 150000), (150000, 200000), (200000, 300000),
        (300000, 400000), (400000, 500000), (500000, 700000), (700000, 1000000),
        (1000000, 2000000), (2000000, 10000000),
    ]

    for f_min, f_max in follower_ranges:
        if f_max >= 10000:
            label = f"{f_min//1000}K~{f_max//1000}K"
        else:
            label = f"{f_min}~{f_max}"

        print(f"\nğŸ“Š íŒ”ë¡œì›Œ {label}")

        for page in range(1, 51):  # ìµœëŒ€ 50í˜ì´ì§€
            result = crawler.search_influencers(
                page=page,
                size=50,
                sort_by="isi",
                sort_order="desc",
                follower_min=f_min,
                follower_max=f_max,
            )

            items = result.get("items", [])
            total_requests += 1

            if not items:
                break

            new_count = 0
            for inf in items:
                username = inf.get("username")
                if username and username not in all_influencers:
                    all_influencers[username] = inf
                    new_count += 1

            if new_count > 0:
                print(f"   í˜ì´ì§€ {page}: +{new_count}ëª… (ì´ {len(all_influencers)}ëª…)")

            if len(items) < 50 or new_count == 0:
                break

            time.sleep(0.1)

    # 3. ì„œë¹„ìŠ¤ íƒ€ì…ë³„ ê¹Šì€ í˜ì´ì§•
    print("\n\nğŸ”¹ 3ë‹¨ê³„: ì„œë¹„ìŠ¤ íƒ€ì…ë³„ ê¹Šì€ í˜ì´ì§•")
    print("-" * 50)

    for services, label in [(["INSTA_BIZ"], "INSTA_BIZ"), (["INSTA_BASIC"], "INSTA_BASIC")]:
        print(f"\nğŸ“Š {label}")
        for page in range(1, 101):
            result = crawler.search_influencers(
                page=page,
                size=50,
                services=services,
                sort_by="isi",
                sort_order="desc",
            )

            items = result.get("items", [])
            total_requests += 1

            if not items:
                break

            new_count = 0
            for inf in items:
                username = inf.get("username")
                if username and username not in all_influencers:
                    all_influencers[username] = inf
                    new_count += 1

            if new_count > 0:
                print(f"   í˜ì´ì§€ {page}: +{new_count}ëª… (ì´ {len(all_influencers)}ëª…)")

            if len(items) < 50:
                break

            time.sleep(0.1)

    # ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
    print(f"\n\n" + "="*60)
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("="*60)
    print(f"ì´ API ìš”ì²­: {total_requests}íšŒ")
    print(f"ê¸°ì¡´ ë°ì´í„°: {initial_count}ëª…")
    print(f"ìµœì¢… ìˆ˜ì§‘: {len(all_influencers)}ëª…")
    print(f"ì‹ ê·œ ì¶”ê°€: +{len(all_influencers) - initial_count}ëª…")

    # ì •ë ¬ í›„ ì €ì¥
    influencer_list = list(all_influencers.values())
    influencer_list.sort(key=lambda x: x.get("isi") or 0, reverse=True)

    with open("tagby_instagram_influencers.json", "w", encoding="utf-8") as f:
        json.dump(influencer_list, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: tagby_instagram_influencers.json")

    # í†µê³„
    print(f"\nğŸ“ˆ ì„œë¹„ìŠ¤ë³„:")
    services_stat = {}
    for inf in influencer_list:
        svc = inf.get("service", "Unknown")
        services_stat[svc] = services_stat.get(svc, 0) + 1
    for svc, count in sorted(services_stat.items(), key=lambda x: -x[1]):
        print(f"   {svc}: {count}ëª…")

    print(f"\nğŸ“Š íŒ”ë¡œì›Œ êµ¬ê°„ë³„:")
    follower_stats = {"~1ë§Œ": 0, "1ë§Œ~5ë§Œ": 0, "5ë§Œ~10ë§Œ": 0, "10ë§Œ~50ë§Œ": 0, "50ë§Œ+": 0}
    for inf in influencer_list:
        fc = inf.get("follower_count") or 0
        if fc < 10000:
            follower_stats["~1ë§Œ"] += 1
        elif fc < 50000:
            follower_stats["1ë§Œ~5ë§Œ"] += 1
        elif fc < 100000:
            follower_stats["5ë§Œ~10ë§Œ"] += 1
        elif fc < 500000:
            follower_stats["10ë§Œ~50ë§Œ"] += 1
        else:
            follower_stats["50ë§Œ+"] += 1
    for label, count in follower_stats.items():
        print(f"   {label}: {count}ëª…")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = TagbyCrawler(
        email="rupy1008@gmail.com",
        password="KM1178km!@"
    )

    # ë¡œê·¸ì¸
    if not crawler.login():
        return

    # ì‚¬ìš©ì ì •ë³´ í™•ì¸
    me = crawler.get_me()
    if me:
        print(f"\nğŸ‘¤ ë¡œê·¸ì¸ ì‚¬ìš©ì: {me.get('name')} ({me.get('company_name')})")

    # ë‹¤ì–‘í•œ í•„í„° ì¡°í•©ìœ¼ë¡œ í¬ë¡¤ë§
    print("\n" + "="*60)
    print("ğŸš€ í™•ì¥ í•„í„°ë¡œ ì¸í”Œë£¨ì–¸ì„œ í¬ë¡¤ë§")
    print("="*60)

    all_influencers = {}  # usernameì„ í‚¤ë¡œ ì‚¬ìš©í•´ì„œ ì¤‘ë³µ ì œê±°

    # íŒ”ë¡œì›Œ ë²”ìœ„ (ë” ì„¸ë¶„í™”)
    follower_ranges = [
        (0, 5000),          # ~5ì²œ
        (5000, 10000),      # 5ì²œ~1ë§Œ
        (10000, 30000),     # 1ë§Œ~3ë§Œ
        (30000, 50000),     # 3ë§Œ~5ë§Œ
        (50000, 100000),    # 5ë§Œ~10ë§Œ
        (100000, 300000),   # 10ë§Œ~30ë§Œ
        (300000, 500000),   # 30ë§Œ~50ë§Œ
        (500000, 1000000),  # 50ë§Œ~100ë§Œ
        (1000000, 9999999), # 100ë§Œ ì´ìƒ
    ]

    # ì •ë ¬ ì˜µì…˜
    sort_options = [
        ("isi", "desc"),           # ISI ë†’ì€ìˆœ
        ("isi", "asc"),            # ISI ë‚®ì€ìˆœ
        ("follower_count", "desc"), # íŒ”ë¡œì›Œ ë§ì€ìˆœ
        ("follower_count", "asc"),  # íŒ”ë¡œì›Œ ì ì€ìˆœ
        ("avg_reach", "desc"),     # í‰ê· ë„ë‹¬ ë†’ì€ìˆœ
        ("avg_reach", "asc"),      # í‰ê· ë„ë‹¬ ë‚®ì€ìˆœ
    ]

    # ìƒë…„ì›”ì¼ ë²”ìœ„ (ì„¸ëŒ€ë³„)
    birth_ranges = [
        ("1970-01-01", "1979-12-31", "70ë…„ëŒ€"),
        ("1980-01-01", "1989-12-31", "80ë…„ëŒ€"),
        ("1990-01-01", "1994-12-31", "90ë…„ëŒ€ ì´ˆ"),
        ("1995-01-01", "1999-12-31", "90ë…„ëŒ€ í›„ë°˜"),
        ("2000-01-01", "2004-12-31", "00ë…„ëŒ€ ì´ˆ"),
        ("2005-01-01", "2010-12-31", "05ë…„ ì´í›„"),
    ]

    # ì„œë¹„ìŠ¤ íƒ€ì… (ê°œë³„)
    service_types = [
        (["INSTA_BIZ"], "Instagram Business"),
        (["INSTA_BASIC"], "Instagram Basic"),
    ]

    total_requests = 0

    # 1ë‹¨ê³„: ê¸°ë³¸ íŒ”ë¡œì›Œ + ì •ë ¬ ì¡°í•©
    print("\n\nğŸ”¹ 1ë‹¨ê³„: íŒ”ë¡œì›Œ ë²”ìœ„ + ì •ë ¬ ì¡°í•©")
    print("-" * 50)
    for f_min, f_max in follower_ranges:
        if f_max >= 10000:
            range_label = f"{f_min//10000}ë§Œ~{f_max//10000}ë§Œ" if f_min >= 10000 else f"~{f_max//10000}ë§Œ"
        else:
            range_label = f"{f_min}~{f_max}"

        for sort_by, sort_order in sort_options:
            print(f"\nğŸ“Š íŒ”ë¡œì›Œ {range_label} | ì •ë ¬: {sort_by} {sort_order}")

            for page in range(1, 6):  # ìµœëŒ€ 5í˜ì´ì§€
                result = crawler.search_influencers(
                    page=page,
                    size=50,  # í˜ì´ì§€ë‹¹ 50ê°œë¡œ ì¦ê°€
                    services=["INSTA_BIZ", "INSTA_BASIC"],
                    sort_by=sort_by,
                    sort_order=sort_order,
                    follower_min=f_min,
                    follower_max=f_max,
                )

                items = result.get("items", [])
                total_requests += 1

                if not items:
                    break

                new_count = 0
                for inf in items:
                    username = inf.get("username")
                    if username and username not in all_influencers:
                        all_influencers[username] = inf
                        new_count += 1

                if new_count > 0:
                    print(f"   í˜ì´ì§€ {page}: +{new_count}ëª… (ì´ {len(all_influencers)}ëª…)")

                if len(items) < 50 or new_count == 0:
                    break

                time.sleep(0.15)

    # 2ë‹¨ê³„: ìƒë…„ì›”ì¼ ë²”ìœ„ë³„
    print("\n\nğŸ”¹ 2ë‹¨ê³„: ì„¸ëŒ€ë³„ í¬ë¡¤ë§")
    print("-" * 50)
    for b_min, b_max, label in birth_ranges:
        for sort_by, sort_order in [("isi", "desc"), ("follower_count", "desc")]:
            print(f"\nğŸ“Š ì„¸ëŒ€: {label} | ì •ë ¬: {sort_by} {sort_order}")

            for page in range(1, 6):
                result = crawler.search_influencers(
                    page=page,
                    size=50,
                    services=["INSTA_BIZ", "INSTA_BASIC"],
                    sort_by=sort_by,
                    sort_order=sort_order,
                    birth_min=b_min,
                    birth_max=b_max,
                )

                items = result.get("items", [])
                total_requests += 1

                if not items:
                    break

                new_count = 0
                for inf in items:
                    username = inf.get("username")
                    if username and username not in all_influencers:
                        all_influencers[username] = inf
                        new_count += 1

                if new_count > 0:
                    print(f"   í˜ì´ì§€ {page}: +{new_count}ëª… (ì´ {len(all_influencers)}ëª…)")

                if len(items) < 50 or new_count == 0:
                    break

                time.sleep(0.15)

    # 3ë‹¨ê³„: ì„œë¹„ìŠ¤ íƒ€ì…ë³„ + íŒ”ë¡œì›Œ ì¡°í•©
    print("\n\nğŸ”¹ 3ë‹¨ê³„: ì„œë¹„ìŠ¤ íƒ€ì…ë³„ í¬ë¡¤ë§")
    print("-" * 50)
    for services, svc_label in service_types:
        for f_min, f_max in follower_ranges[:5]:  # íŒ”ë¡œì›Œ ë‚®ì€ êµ¬ê°„ ì§‘ì¤‘
            if f_max >= 10000:
                range_label = f"{f_min//10000}ë§Œ~{f_max//10000}ë§Œ" if f_min >= 10000 else f"~{f_max//10000}ë§Œ"
            else:
                range_label = f"{f_min}~{f_max}"

            print(f"\nğŸ“Š {svc_label} | íŒ”ë¡œì›Œ {range_label}")

            for page in range(1, 6):
                result = crawler.search_influencers(
                    page=page,
                    size=50,
                    services=services,
                    sort_by="isi",
                    sort_order="desc",
                    follower_min=f_min,
                    follower_max=f_max,
                )

                items = result.get("items", [])
                total_requests += 1

                if not items:
                    break

                new_count = 0
                for inf in items:
                    username = inf.get("username")
                    if username and username not in all_influencers:
                        all_influencers[username] = inf
                        new_count += 1

                if new_count > 0:
                    print(f"   í˜ì´ì§€ {page}: +{new_count}ëª… (ì´ {len(all_influencers)}ëª…)")

                if len(items) < 50 or new_count == 0:
                    break

                time.sleep(0.15)

    # 4ë‹¨ê³„: ê´‘ê³ ì§€ìˆ˜ ì •ë ¬
    print("\n\nğŸ”¹ 4ë‹¨ê³„: ê´‘ê³ ì§€ìˆ˜(ad_rate) ì •ë ¬")
    print("-" * 50)
    for sort_order in ["desc", "asc"]:
        print(f"\nğŸ“Š ê´‘ê³ ì§€ìˆ˜ {sort_order}")
        for page in range(1, 11):
            result = crawler.search_influencers(
                page=page,
                size=50,
                services=["INSTA_BIZ", "INSTA_BASIC"],
                sort_by="ad_rate",
                sort_order=sort_order,
            )

            items = result.get("items", [])
            total_requests += 1

            if not items:
                break

            new_count = 0
            for inf in items:
                username = inf.get("username")
                if username and username not in all_influencers:
                    all_influencers[username] = inf
                    new_count += 1

            if new_count > 0:
                print(f"   í˜ì´ì§€ {page}: +{new_count}ëª… (ì´ {len(all_influencers)}ëª…)")

            if len(items) < 50 or new_count == 0:
                break

            time.sleep(0.15)

    print(f"\n\nì´ API ìš”ì²­: {total_requests}íšŒ")

    print(f"\n" + "="*60)
    print(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(all_influencers)}ëª… (ì¤‘ë³µ ì œê±°ë¨)")
    print("="*60)

    # ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ISI ìˆœìœ¼ë¡œ ì •ë ¬
    influencer_list = list(all_influencers.values())
    influencer_list.sort(key=lambda x: x.get("isi") or 0, reverse=True)

    # ìƒìœ„ 30ëª… ì¶œë ¥
    print(f"\nğŸ“‹ ì¸í”Œë£¨ì–¸ì„œ ìƒìœ„ 30ëª… (ISI ìˆœ):")
    print("-" * 130)
    print(f"{'#':>3} | {'Username':<25} | {'Service':<12} | {'Followers':>12} | {'Avg Reach':>12} | {'ISI':>10} | {'Ad Rate':>8} | {'Category':<15}")
    print("-" * 130)

    for i, inf in enumerate(influencer_list[:30], 1):
        username = inf.get("username", "N/A")
        service = inf.get("service", "N/A")
        follower = inf.get("follower_count") or 0
        avg_reach = inf.get("avg_reach") or 0
        isi = inf.get("isi") or 0
        ad_rate = inf.get("ad_rate") or 0
        category = inf.get("sns_label", {}).get("category", [])
        cat_str = "/".join(category[:2]) if category else "-"
        print(f"{i:>3} | @{username:<24} | {service:<12} | {follower:>12,} | {avg_reach:>12,} | {isi:>10.0f} | {ad_rate:>7}% | {cat_str:<15}")

    print("-" * 130)

    # í†µê³„
    print(f"\nğŸ“ˆ í†µê³„:")
    services = {}
    for inf in influencer_list:
        svc = inf.get("service", "Unknown")
        services[svc] = services.get(svc, 0) + 1
    for svc, count in sorted(services.items(), key=lambda x: -x[1]):
        print(f"   {svc}: {count}ëª…")

    # íŒ”ë¡œì›Œ êµ¬ê°„ë³„ í†µê³„
    print(f"\nğŸ“Š íŒ”ë¡œì›Œ êµ¬ê°„ë³„:")
    follower_stats = {"~1ë§Œ": 0, "1ë§Œ~5ë§Œ": 0, "5ë§Œ~10ë§Œ": 0, "10ë§Œ~50ë§Œ": 0, "50ë§Œ+": 0}
    for inf in influencer_list:
        fc = inf.get("follower_count") or 0
        if fc < 10000:
            follower_stats["~1ë§Œ"] += 1
        elif fc < 50000:
            follower_stats["1ë§Œ~5ë§Œ"] += 1
        elif fc < 100000:
            follower_stats["5ë§Œ~10ë§Œ"] += 1
        elif fc < 500000:
            follower_stats["10ë§Œ~50ë§Œ"] += 1
        else:
            follower_stats["50ë§Œ+"] += 1
    for label, count in follower_stats.items():
        print(f"   {label}: {count}ëª…")

    # ê²°ê³¼ ì €ì¥
    crawler.save_to_json(influencer_list, "tagby_instagram_influencers.json")


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "deep":
        main_deep_pagination()
    else:
        main_deep_pagination()  # ê¸°ë³¸ìœ¼ë¡œ ê¹Šì€ í˜ì´ì§• ì‚¬ìš©
